# -*- coding: utf-8 -*-
"""190050k-ml-project-layer-09.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gxleYFZwHbC8jD6cDWf0DfI6_-ZF6sri
"""

import pandas as pd
from sklearn.model_selection import GridSearchCV
from sklearn import metrics
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
from sklearn.preprocessing import RobustScaler
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif

from imblearn.over_sampling import RandomOverSampler

from imblearn.under_sampling import RandomUnderSampler

from imblearn.over_sampling import SMOTE

from imblearn.pipeline import Pipeline as iPipeline

from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

import shap

import seaborn as sns
import matplotlib.pyplot as plt

"""# Helping Functions"""

import pickle

def saveModel(fileName,model):
    with open(f'{fileName}.pickle',"wb") as file:
        pickle.dump(model,file)

def loadModel(modelFile):
    with open(modelFile, 'rb') as file:
        model = pickle.load(file)
        return model

"""# Load the data"""

# Load the train data
train_data = pd.read_csv('/kaggle/input/speech-based-classification-layer-9/train.csv')

train_data

# Load the valid data
valid_data = pd.read_csv('/kaggle/input/speech-based-classification-layer-9/valid.csv')

valid_data

# Load the valid data
test_data = pd.read_csv('/kaggle/input/speech-based-classification-layer-9/test.csv')

test_data

# Split data into features (X) and labels (Y) for each label for train dataset
X_train = train_data.loc[:, 'feature_1':'feature_768']
Y_speaker_train = train_data['label_1']
Y_age_train = train_data['label_2']
Y_gender_train = train_data['label_3']
Y_accent_train = train_data['label_4']

# Split data into features (X) and labels (Y) for each label for valid dataset
X_valid = valid_data.loc[:, 'feature_1':'feature_768']
Y_speaker_valid = valid_data['label_1']
Y_age_valid = valid_data['label_2']
Y_gender_valid = valid_data['label_3']
Y_accent_valid = valid_data['label_4']

X_train

"""Now lets find best classification model for each label. Lets classify Label 1 first.

# Find best classifier model for Label 1

Check the nature of Label 1
"""

Y_speaker_train.describe()

"""Remove outliers if there are any of them"""

robustScaler= RobustScaler()
X_train = pd.DataFrame(robustScaler.fit_transform(X_train))
X_valid = pd.DataFrame(robustScaler.fit_transform(X_valid))

"""Next check are there any null values in the Label 1 column."""

Y_speaker_train_df = Y_speaker_train.to_frame()

null_values = Y_speaker_train_df['label_1'].isnull()
null_count = null_values.sum()

print(f"Number of null values in 'label_1': {null_count}")

"""Therefore there is not any null values.
Now Let's check the value distribution with a graph.
"""

# Create a countplot using Seaborn
plt.figure(figsize=(15, 12))
sns.countplot(data=Y_speaker_train_df, x='label_1')
plt.xlabel('Label')
plt.ylabel('Count')
plt.title('Number of Occurrences for label_1')
plt.show()

"""No bias in the dataset.

Now check the correlation between the features
"""

X_train_label_1 = X_train.copy()
X_valid_label_1 = X_valid.copy()
X_test_label_1 = test_data.loc[:, 'feature_1':'feature_768'].copy()

corr_before_feature_engineering_label_1 = X_train_label_1.corr()

plt.figure(figsize=(16, 16))
plt.xticks(rotation=90)
sns.heatmap(corr_before_feature_engineering_label_1, cmap="YlGnBu")
plt.savefig("All_data.png")

"""Now lets find best classifier model to predict Label 1"""

score_knn = cross_val_score(KNeighborsClassifier(n_neighbors=10), X_train_label_1, Y_speaker_train,cv=3)
score_svm = cross_val_score(svm.SVC(kernel='linear', probability=True, class_weight='balanced'),X_train_label_1, Y_speaker_train,cv=3)
score_rfc = cross_val_score(RandomForestClassifier(n_estimators=10),X_train_label_1, Y_speaker_train, cv=3)
print(f"score for knn: {score_knn}")
print(f"score for svm: {score_svm}")
print(f"score_rfc:{score_rfc}")

"""Therefore SVM has the best accuracy

Now lets predict with SVC model
"""

svc_label_1 = svm.SVC(kernel='linear')
svc_label_1.fit(X_train_label_1, Y_speaker_train_df)

y_predict_label_1 = svc_label_1.predict(X_valid_label_1)

print(metrics.confusion_matrix(Y_speaker_valid, y_predict_label_1))
print(metrics.accuracy_score(Y_speaker_valid, y_predict_label_1))
print(metrics.precision_score(Y_speaker_valid, y_predict_label_1, average = 'weighted'))
print(metrics.recall_score(Y_speaker_valid, y_predict_label_1, average = 'weighted'))

"""Lets do the hyper parameter tuning and find the best model by changing the parameters. I use PCA for feature reduction task."""

# Create a pipeline with PCA, SVM, and grid search for hyperparameter tuning
pipeline = Pipeline([
    ('pca', PCA()),
    ('svm', svm.SVC())
])

"""Check by changinge following parameters"""

# Define the parameter grid for grid search
param_grid = {
    'pca__n_components': [0.94],  # Number of principal components to retain
    'svm__C': [0.1, 1, 10, 28, 30],         # SVM regularization parameter
    'svm__kernel': ['linear', 'rbf']         # SVM kernel
}

# Perform grid search with cross-validation
grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=3, scoring='accuracy')
grid_search.fit(X_train, Y_speaker_train)

# Get the best model and its hyperparameters
best_model = grid_search.best_estimator_
best_params = grid_search.best_params_
best_score = grid_search.best_score_

best_params

# Evaluate the best model on the test set
accuracy = best_model.score(X_valid, Y_speaker_valid)

y_pred_with_grid_search_label_1 = best_model.predict(X_valid)

print(metrics.confusion_matrix(Y_speaker_valid, y_pred_with_grid_search_label_1))
print(metrics.accuracy_score(Y_speaker_valid, y_pred_with_grid_search_label_1))
print(metrics.precision_score(Y_speaker_valid, y_pred_with_grid_search_label_1, average = 'weighted'))
print(metrics.recall_score(Y_speaker_valid, y_pred_with_grid_search_label_1, average = 'weighted'))

"""Use explainable AI"""

def svc_predictor_label_1(X):
    return best_model.decision_function(X)

explainer_label_1 = shap.Explainer(svc_predictor_label_1, X_train_label_1)
shap_values_label_1 = explainer_label_1.shap_values(X_valid_label_1)
shap.summary_plot(shap_values_label_1, X_valid_label_1, feature_names=X.columns)

saveModel("best_pca__n_0.94_svm__C_28_svm__kernel_rbf", best_model)

"""Create CSV output file"""

y_predict_final_label_1 = best_model.predict(X_test_label_1)

y_predict_final_label_1.shape

y_predict_final_label_1_series = pd.Series(y_predict_final_label_1, name='Predicted_Label_1')

final_result_label_1 = pd.concat([test_data['ID'], y_predict_final_label_1_series], axis=1)
final_result_label_1

final_result_label_1.to_csv('190050K layer 9 label 1.csv', index=False)



"""Let find classification model for Label 2

# Find best classifier model for Label 2

Check the nature of Label 2
"""

Y_age_train.describe()

robustScaler= RobustScaler()
X_train = pd.DataFrame(robustScaler.fit_transform(X_train))
X_valid = pd.DataFrame(robustScaler.fit_transform(X_valid))

"""Next check are there any null values in the Label 2 column."""

Y_age_train_df = Y_age_train.to_frame()

null_values = Y_age_train_df['label_2'].isnull()
null_count = null_values.sum()

print(f"Number of null values in 'label_2': {null_count}")

print(f"Percentage of null values in 'label_2': {null_count/X_train.shape[0]*100}%")

"""Therefore the percentage of NULL values are low. Therefore I can elemenate these rows for future calculations."""

preprocessed_train_data = train_data[train_data['label_2'].notna()]
preprocessed_valid_data = valid_data[train_data['label_2'].notna()]

preprocessed_train_data

preprocessed_valid_data

X_train = preprocessed_train_data.loc[:, 'feature_1':'feature_768']
Y_age_train = preprocessed_train_data['label_2']

X_valid = preprocessed_valid_data.loc[:, 'feature_1':'feature_768']
Y_age_valid = preprocessed_valid_data['label_2']

"""Now Let's check the value distribution with a graph."""

plt.figure(figsize=(15, 12))
sns.countplot(data=preprocessed_train_data, x='label_2')
plt.xlabel('Label')
plt.ylabel('Count')
plt.title('Number of Occurrences for label_2')
plt.show()

"""Now check the correlation between the features"""

X_train_label_2 = X_train.copy()
X_valid_label_2 = X_valid.copy()
X_test_label_2 = test_data.loc[:, 'feature_1':'feature_768'].copy()

corr_before_feature_engineering_label_2 = X_train_label_2.corr()

plt.figure(figsize=(14, 14))
plt.xticks(rotation=90)
sns.heatmap(corr_before_feature_engineering_label_2, cmap="YlGnBu")

"""Now select best model to predict 'Label_2'"""

score_knn = cross_val_score(KNeighborsClassifier(n_neighbors=5), X_train_label_2, Y_age_train,cv=3)
score_svm = cross_val_score(svm.SVC(kernel='linear', probability=True, class_weight='balanced'),X_train_label_2, Y_age_train,cv=3)
score_rfc = cross_val_score(RandomForestClassifier(n_estimators=5),X_train_label_2, Y_age_train, cv=3)
print(f"Score for knn: {score_knn}")
print(f"Score for svm: {score_svm}")
print(f"Score for RFC:{score_rfc}")

"""Therefore best results are with KNN model. Lets consider both KNN and SVM models for predict Label 2.

First lets predict with KNN model
"""

knn_label_2 = KNeighborsClassifier(n_neighbors=5)
knn_label_2.fit(X_train_label_2, Y_age_train)

y_predict_label_2 = knn_label_2.predict(X_valid_label_2)

print(metrics.confusion_matrix(Y_age_valid, y_predict_label_2))
print(metrics.accuracy_score(Y_age_valid, y_predict_label_2))
print(metrics.precision_score(Y_age_valid, y_predict_label_2, average = 'weighted'))
print(metrics.recall_score(Y_age_valid, y_predict_label_2, average = 'weighted'))



"""Lets do the hyper parameter tuning and find the best model by changing the parameters. I use PCA for feature reduction task"""

pipeline = Pipeline([
    ('pca', PCA()),
    ('knn', KNeighborsClassifier())
])

param_grid = {
    'pca__n_components': [0.93, 0.96, 0.99],  # Number of principal components
    'knn__n_neighbors': [3, 4, 5, 6, 7],    # Number of neighbors for KNN
    'knn__weights': ['distance'],
}

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=3, scoring='accuracy')
grid_search.fit(X_train, Y_age_train)

# Get the best model and its hyperparameters
best_model = grid_search.best_estimator_
best_params = grid_search.best_params_
best_score = grid_search.best_score_

best_params

accuracy = best_model.score(X_valid, Y_age_valid)

accuracy

y_pred_with_grid_search_label_2 = best_model.predict(X_valid)

print(metrics.confusion_matrix(Y_age_valid, y_pred_with_grid_search_label_2))
print(metrics.accuracy_score(Y_age_valid, y_pred_with_grid_search_label_2))
print(metrics.precision_score(Y_age_valid, y_pred_with_grid_search_label_2, average = 'weighted'))
print(metrics.recall_score(Y_age_valid, y_pred_with_grid_search_label_2, average = 'weighted'))



"""Now lets predict with SVM model"""

svc_label_2 = svm.SVC(kernel='rbf')
svc_label_2.fit(X_train_label_2, Y_age_train)

y_predict_label_2 = svc_label_2.predict(X_valid_label_2)

print(metrics.confusion_matrix(Y_age_valid, y_predict_label_2))
print(metrics.accuracy_score(Y_age_valid, y_predict_label_2))
print(metrics.precision_score(Y_age_valid, y_predict_label_2, average = 'weighted'))
print(metrics.recall_score(Y_age_valid, y_predict_label_2, average = 'weighted'))

"""Lets do the hyper parameter tuning and find the best model by changing the parameters. I use PCA for feature reduction task"""

pipeline = Pipeline([
    ('pca', PCA()),
    ('svm', svm.SVC())
])

"""Check the perfoermance of the model by changing parameters in the following."""

param_grid = {
    'pca__n_components': [0.96],
    'svm__C': [0.1, 1, 10, 20, 50, 55],
    'svm__kernel': ['rbf'],
    'svm__class_weight': ['balanced'],
    'svm__probability': [True]
}

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=3, scoring='accuracy')
grid_search.fit(X_train, Y_age_train)

best_model = grid_search.best_estimator_
best_params = grid_search.best_params_
best_score = grid_search.best_score_

best_params

accuracy = best_model.score(X_valid, Y_age_valid)

accuracy

y_pred_with_grid_search_label_2 = best_model.predict(X_valid)

print(metrics.confusion_matrix(Y_age_valid, y_pred_with_grid_search_label_2))
print(metrics.accuracy_score(Y_age_valid, y_pred_with_grid_search_label_2))
print(metrics.precision_score(Y_age_valid, y_pred_with_grid_search_label_2, average = 'weighted'))
print(metrics.recall_score(Y_age_valid, y_pred_with_grid_search_label_2, average = 'weighted'))

"""Therefore, I could get a best result for SVM model by using hyper parameter tuning method.

Use explainable AI for label 2
"""

def svc_predictor_label_2(X):
    return best_model.decision_function(X)

explainer_label_2 = shap.Explainer(svc_predictor_label_2, X_train_label_2)
shap_values_label_2 = explainer_label_2.shap_values(X_valid_label_2)
shap.summary_plot(shap_values_label_2, X_valid_label_2, feature_names=X.columns)

y_predict_final_label_2 = best_model.predict(X_test_label_2)

"""# Find best classifier model for Label 3"""

train_data = pd.read_csv('/kaggle/input/speech-based-classification-layer-9/train.csv')
valid_data = pd.read_csv('/kaggle/input/speech-based-classification-layer-9/valid.csv')

# load data in Train dataset
X_train = train_data.loc[:, 'feature_1':'feature_768']
Y_gender_train = train_data['label_3']

# load data in Valid dataset
X_valid = valid_data.loc[:, 'feature_1':'feature_768']
Y_gender_valid = valid_data['label_3']

"""Check the nature of Label 3"""

Y_gender_train_df = Y_gender_train.to_frame()

Y_gender_train_df.describe()

robustScaler= RobustScaler()
X_train = pd.DataFrame(robustScaler.fit_transform(X_train))
X_valid = pd.DataFrame(robustScaler.fit_transform(X_valid))

"""Next check are there any null values in the Label 3 column."""

null_values = Y_gender_train_df['label_3'].isnull()
null_count = null_values.sum()

print(f"Number of null values in 'label_3': {null_count}")

"""Therefore there is not any null values. Now Let's check the value distribution with a graph."""

plt.figure(figsize=(15, 12))
sns.countplot(data=Y_gender_train_df, x='label_3')
plt.xlabel('Label')
plt.ylabel('Count')
plt.title('Number of Occurrences for label_3')
plt.show()

"""There is a bias in data to value "1". Lets try to oversample this"""

# # Initialize the RandomOverSampler
ros = RandomOverSampler(random_state=42)

# Fit and apply the resampling to your data
X_train_resampled, Y_gender_train_resampled = ros.fit_resample(X_train, Y_gender_train)

Y_gender_train_df = Y_gender_train_resampled.to_frame()

plt.figure(figsize=(15, 12))
sns.countplot(data=Y_gender_train_df, x='label_3')
plt.xlabel('Label')
plt.ylabel('Count')
plt.title('Number of Occurrences for label_3')
plt.show()

# # Update the values with resampled data
Y_gender_train = Y_gender_train_resampled

X_train_label_3 = X_train_resampled.copy()
X_valid_label_3 = X_valid.copy()
X_test_label_3 = test_data.loc[:, 'feature_1':'feature_768'].copy()

corr_before_feature_engineering_label_3 = X_train_label_3.corr()

plt.figure(figsize=(14, 14))
plt.xticks(rotation=90)
sns.heatmap(corr_before_feature_engineering_label_3, cmap="YlGnBu")

"""Now select best model to predict 'Label_3'"""

score_knn = cross_val_score(KNeighborsClassifier(n_neighbors=5), X_train_label_3, Y_gender_train,cv=3)
score_svm = cross_val_score(svm.SVC(kernel='linear', probability=True, class_weight='balanced'),X_train_label_3, Y_gender_train,cv=3)
score_rfc = cross_val_score(RandomForestClassifier(n_estimators=5),X_train_label_3, Y_gender_train, cv=3)
print(f"Score for knn: {score_knn}")
print(f"Score for svm: {score_svm}")
print(f"Score for RFC:{score_rfc}")

"""Therefore SVM model has the best value. Let's work with this model for further fine tuning tasks."""

svc_label_3 = svm.SVC(kernel='linear', class_weight = 'balanced', probability = True)
svc_label_3.fit(X_train_label_3, Y_gender_train_df)

y_predict_label_3 = svc_label_3.predict(X_valid_label_3)

print(metrics.confusion_matrix(Y_gender_valid, y_predict_label_3))
print(metrics.accuracy_score(Y_gender_valid, y_predict_label_3))
print(metrics.precision_score(Y_gender_valid, y_predict_label_3, average = 'weighted'))
print(metrics.recall_score(Y_gender_valid, y_predict_label_3, average = 'weighted'))

"""Let's try to do the hyper parameter fine tuning with SVM model. I used PCA for fearure reduction."""

pipeline = Pipeline([
    ('pca', PCA()),
    ('svm', svm.SVC())
])

param_grid = {
    'pca__n_components': [0.94],  # Number of principal components to retain
    'svm__C': [0.1, 1, 5, 9, 10],         # SVM regularization parameter
    'svm__kernel': ['linear', 'rbf'],         # SVM kernel
    'svm__class_weight': ['balanced'],
#     'svm__probability': [True]
}

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=3, scoring='accuracy')
grid_search.fit(X_train_label_3, Y_gender_train)

best_model = grid_search.best_estimator_
best_params = grid_search.best_params_
best_score = grid_search.best_score_

best_params

accuracy = best_model.score(X_valid, Y_gender_valid)

accuracy

y_pred_with_grid_search_label_3 = best_model.predict(X_valid)

print(metrics.confusion_matrix(Y_gender_valid, y_pred_with_grid_search_label_3))
print(metrics.accuracy_score(Y_gender_valid, y_pred_with_grid_search_label_3))
print(metrics.precision_score(Y_gender_valid, y_pred_with_grid_search_label_3, average = 'weighted'))
print(metrics.recall_score(Y_gender_valid, y_pred_with_grid_search_label_3, average = 'weighted'))

y_predict_final_label_3 = best_model.predict(X_test_label_3)

"""Use explainable AI for label 3"""

def svc_predictor_label_3(X):
    return best_model.decision_function(X)

explainer_label_3 = shap.Explainer(svc_predictor_label_3, X_train_label_3)
shap_values_label_3 = explainer_label_3.shap_values(X_valid_label_3)
shap.summary_plot(shap_values_label_3, X_valid_label_3, feature_names=X.columns)



"""# Find best classifier model for Label 4"""

# load data in Train dataset
X_train = train_data.loc[:, 'feature_1':'feature_768']
Y_accent_train = train_data['label_4']

# load data in Valid dataset
X_valid = valid_data.loc[:, 'feature_1':'feature_768']
Y_accent_valid = valid_data['label_4']

Y_accent_train_df = Y_accent_train.to_frame()

"""Let's check the nature of the Label 4"""

Y_accent_train_df.describe()

robustScaler= RobustScaler()
X_train = pd.DataFrame(robustScaler.fit_transform(X_train))
X_valid = pd.DataFrame(robustScaler.fit_transform(X_valid))

"""Next check are there any null values in the Label 4 column."""

null_values = Y_accent_train_df['label_4'].isnull()
null_count = null_values.sum()

print(f"Number of null values in 'label_4': {null_count}")

"""Therefore there is not any null values. Now Let's check the value distribution with a graph."""

plt.figure(figsize=(15, 12))
sns.countplot(data=Y_accent_train_df, x='label_4')
plt.xlabel('Label')
plt.ylabel('Count')
plt.title('Number of Occurrences for label_4')
plt.show()

"""The dataset for Label 4 biased to value '6'. We need consider this for future evaluations.

Resample data
"""

# Apply Random Under-sampling
under_sampler = RandomOverSampler(sampling_strategy='auto', random_state=42)

# Fit and apply the pipeline to your data
X_train_resampled, Y_accent_resampled = under_sampler.fit_resample(X_train, Y_accent_train)

"""Value distribution after resampling"""

plt.figure(figsize=(15, 12))
sns.countplot(data=Y_accent_resampled.to_frame(), x='label_4')
plt.xlabel('Label')
plt.ylabel('Count')
plt.title('Number of Occurrences for label_4')
plt.show()

"""Now check the correlation between the features"""

X_train_label_4 = X_train_resampled.copy()
X_valid_label_4 = X_valid.copy()
X_test_label_4 = test_data.loc[:, 'feature_1':'feature_768'].copy()

corr_before_feature_engineering_label_4 = X_train_label_4.corr()

plt.figure(figsize=(14, 14))
plt.xticks(rotation=90)
sns.heatmap(corr_before_feature_engineering_label_4, cmap="YlGnBu")

"""Lets select best model for classification in Label 4"""

score_knn = cross_val_score(KNeighborsClassifier(n_neighbors=10), X_train_label_4, Y_accent_resampled,cv=3)
score_svm = cross_val_score(svm.SVC(kernel='linear', probability=True, class_weight='balanced'),X_train_label_4, Y_accent_resampled,cv=3)
print(f"Score for knn: {score_knn}")
print(f"Score for svm: {score_svm}")

"""Because of both scores are close, lets consider both approaches

First lets consider about SVM classification
"""

svc_label_4 = svm.SVC(kernel='rbf', class_weight = 'balanced', probability = True)
svc_label_4.fit(X_train_label_4, Y_accent_resampled)

y_predict_label_4 = svc_label_4.predict(X_valid_label_4)

print(metrics.confusion_matrix(Y_accent_valid, y_predict_label_4))
print(metrics.accuracy_score(Y_accent_valid, y_predict_label_4))
print(metrics.precision_score(Y_accent_valid, y_predict_label_4, average = 'weighted'))
print(metrics.recall_score(Y_accent_valid, y_predict_label_4, average = 'weighted'))

"""Lets do hyper parameter fine tuning for find best model for Label 4. I used PCA for feature reduction."""

pipeline = Pipeline([
    ('pca', PCA()),
    ('svm', svm.SVC())
])

"""Changed values and found model with the best resuls"""

param_grid = {
    'pca__n_components': [0.95],  # Number of principal components to retain
    'svm__C': [0.1, 1, 10],         # SVM regularization parameter
    'svm__kernel': ['rbf'],         # SVM kernel
    'svm__class_weight': ['balanced'],
#     'svm__probability': [True]
}

# Perform grid search with cross-validation
grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=3, scoring='accuracy')
grid_search.fit(X_train_resampled, Y_accent_resampled)

best_model = grid_search.best_estimator_
best_params = grid_search.best_params_
best_score = grid_search.best_score_

best_params

accuracy = best_model.score(X_valid, Y_accent_valid)

y_pred_with_grid_search_label_4 = best_model.predict(X_valid)

print(metrics.confusion_matrix(Y_accent_valid, y_pred_with_grid_search_label_4))
print(metrics.accuracy_score(Y_accent_valid, y_pred_with_grid_search_label_4))
print(metrics.precision_score(Y_accent_valid, y_pred_with_grid_search_label_4, average = 'weighted'))
print(metrics.recall_score(Y_accent_valid, y_pred_with_grid_search_label_4, average = 'weighted'))

"""Use explainable AI for label 4"""

def svc_predictor_label_4(X):
    return best_model.decision_function(X)

explainer_label_4 = shap.Explainer(svc_predictor_label_4, X_train_label_4)
shap_values_label_4 = explainer_label_4.shap_values(X_valid_label_4)
shap.summary_plot(shap_values_label_4, X_valid_label_4, feature_names=X.columns)

saveModel("pca__n_0.90_svm__C_18_svm__kernel_rbf", best_model)

y_predict_final_label_4 = best_model.predict(X_test_label_4)

"""Now lets classify with KNN for label 4"""

knn_label_4 = KNeighborsClassifier(n_neighbors=5)
knn_label_4.fit(X_train_resampled, Y_accent_resampled)

y_predict_label_4 = knn_label_4.predict(X_valid_label_4)

print(metrics.confusion_matrix(Y_accent_valid, y_predict_label_4))
print(metrics.accuracy_score(Y_accent_valid, y_predict_label_4))
print(metrics.precision_score(Y_accent_valid, y_predict_label_4, average = 'weighted'))
print(metrics.recall_score(Y_accent_valid, y_predict_label_4, average = 'weighted'))

"""Lets do the hyper parameter tuning and find the best model by changing the parameters. I use PCA for feature reduction task"""

pipeline = Pipeline([
    ('pca', PCA()),
    ('knn', KNeighborsClassifier())
])

"""Check best model by changinge following parameters"""

# Define the parameter grid for grid search
param_grid = {
    'pca__n_components': [0.95]
    'knn__n_neighbors': [5, 10, 20],
    'knn__weights': ['distance'],
}

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=3, scoring='accuracy')
grid_search.fit(X_train_resampled, Y_accent_resampled)

best_model_knn = grid_search.best_estimator_
best_params_knn = grid_search.best_params_
best_score_knn = grid_search.best_score_

best_params_knn

accuracy_knn = best_model_knn.score(X_valid, Y_accent_valid)

accuracy_knn

y_pred_with_grid_search_label_4_knn = best_model_knn.predict(X_valid)

print(metrics.confusion_matrix(Y_accent_valid, y_pred_with_grid_search_label_4_knn))
print(metrics.accuracy_score(Y_accent_valid, y_pred_with_grid_search_label_4_knn))
print(metrics.precision_score(Y_accent_valid, y_pred_with_grid_search_label_4_knn, average = 'weighted'))
print(metrics.recall_score(Y_accent_valid, y_pred_with_grid_search_label_4_knn, average = 'weighted'))



"""# Combine All Results"""

final_result_all = pd.concat([test_data['ID'], pd.Series(y_predict_final_label_1, name='label_1'), pd.Series(y_predict_final_label_2, name='label_2'), pd.Series(y_predict_final_label_3, name='label_3'), pd.Series(y_predict_final_label_4, name='label_4')], axis=1)

final_result_all

final_result_all.to_csv('190050K layer 9 new.csv', index=False)